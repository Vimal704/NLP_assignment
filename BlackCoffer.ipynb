{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-rP3Dcwwnat",
        "outputId": "5ef9a11f-2f8a-421b-cd5b-6d692f26070c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "nltk.download('cmudict')\n",
        "from nltk.corpus import cmudict, stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY5REw0Ownaw"
      },
      "outputs": [],
      "source": [
        "#urls=pd.read_csv('Input.xlsx.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ECRr0JGlwnaw"
      },
      "outputs": [],
      "source": [
        "def fectchSaveToFile(url,path):\n",
        "    r=requests.get(url)\n",
        "    with open(path,'w', encoding=\"utf-8\") as f:\n",
        "        f.write(r.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GKfBzBcwnax"
      },
      "source": [
        "Getting Stop words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zZIRdIRewnay"
      },
      "outputs": [],
      "source": [
        "def stop_words_f():\n",
        "    text_data=''\n",
        "    files=os.listdir('stop/')\n",
        "    for file in files:\n",
        "        with open('stop/'+file,'r',encoding='latin-1') as f:\n",
        "            text_data+=f.read()\n",
        "    text_data=re.sub(r'[^\\w\\s\\-]','',text_data).lower()\n",
        "    stop_words=text_data.split()\n",
        "    return stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J_LMb4Vwnay"
      },
      "source": [
        "Getting Positive words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "alvSk5sXwnaz"
      },
      "outputs": [],
      "source": [
        "def Postive_words(stop_words):\n",
        "    with open(\"neg_pos_words/positive-words.txt\",'r') as f:\n",
        "        pos=f.read()\n",
        "    pos=pos.split()\n",
        "\n",
        "    pos_words=[]\n",
        "    for i in pos:\n",
        "        if i not in stop_words:\n",
        "            pos_words.append(i)\n",
        "    return pos_words,pos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ywJ8vY8wnaz"
      },
      "source": [
        "Getting negative words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xwelEVzlwnaz"
      },
      "outputs": [],
      "source": [
        "def Negative_words(stop_words):\n",
        "    with open(\"neg_pos_words/negative-words.txt\",'r',encoding='latin-1') as f:\n",
        "        neg=f.read()\n",
        "    neg=neg.split()\n",
        "\n",
        "    #removing positive words that are atop words\n",
        "    neg_words=[]\n",
        "    for i in neg:\n",
        "        if i not in stop_words:\n",
        "            neg_words.append(i)\n",
        "    return neg_words,neg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNTxCe4Rwnaz"
      },
      "source": [
        "Getting Web articel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b8H-ewGTwnaz"
      },
      "outputs": [],
      "source": [
        "def WebArticle(add):\n",
        "    with open(add,'r', encoding=\"utf8\") as f:\n",
        "        html_doc=f.read()\n",
        "\n",
        "    soup=BeautifulSoup(html_doc,'html.parser')\n",
        "    # print(soup.title.string)\n",
        "    web_article=\"\"\n",
        "    web_article+=soup.title.text\n",
        "    articles=soup.article\n",
        "    scrapped=articles.find_all('p')\n",
        "    for i in scrapped:\n",
        "        web_article+=i.text\n",
        "    # web_article=web_article.lower()\n",
        "    og_web=web_article\n",
        "    web_article=re.sub(r'[^\\w\\s\\-]','',web_article).lower().split()\n",
        "    return web_article,og_web"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGz6U2SIwna0"
      },
      "source": [
        "Removing Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BpcTyFySwna0"
      },
      "outputs": [],
      "source": [
        "def Remove_stop_words(web_article,stop_words):\n",
        "    cleaned=[]\n",
        "    for i in web_article:\n",
        "        if i not in stop_words:\n",
        "            cleaned.append(i)\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPf_g93swna0"
      },
      "source": [
        "Calculating Positive and negative words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "noeIlCJ7wna0"
      },
      "outputs": [],
      "source": [
        "def positive_negative_score(cleaned,pos,neg):\n",
        "    p_score=0\n",
        "    n_score=0\n",
        "    for i in cleaned:\n",
        "        if i in pos:\n",
        "            p_score+=1\n",
        "        if i in neg:\n",
        "            n_score+=1\n",
        "    return p_score,n_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6nPn2T-wna0"
      },
      "source": [
        "Calculating Polarity Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_tJaSnPvwna0"
      },
      "outputs": [],
      "source": [
        "def Polarity_Scroe(p_score,n_score):\n",
        "    polarity_score=(p_score-n_score)/((p_score+n_score)+0.000001)\n",
        "    return polarity_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h7o9Hqvwna0"
      },
      "source": [
        "Calculating Subjectivity Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ufII3kzjwna1"
      },
      "outputs": [],
      "source": [
        "def SubjectivityScore(p_score,n_score,cleaned):\n",
        "    return (p_score+n_score/(len(cleaned)+0.000001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu0ps6nCwna1"
      },
      "source": [
        "#### 2.Analysis of Readability,\n",
        "#### 3.Average Number of Words Per Sentence,\n",
        "#### 4.Conplex Word Count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzWVP_Hlwna1"
      },
      "source": [
        "> Average Word Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oT_C3diFwna1"
      },
      "outputs": [],
      "source": [
        "def AverageSentLength(web_article,og_web):\n",
        "    # web_article=web_article.lower()\n",
        "    total_no_of_words=len(web_article)\n",
        "    no_of_sent=len(nltk.sent_tokenize(og_web))\n",
        "    Avg_sent_len=total_no_of_words/no_of_sent\n",
        "    return Avg_sent_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C4-qDfCwna1"
      },
      "source": [
        ">Percentage Of Complex words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BeGQYFxDwna1"
      },
      "outputs": [],
      "source": [
        "# Referred from stack overflow\n",
        "def syllables(word):\n",
        "    count = 0\n",
        "    vowels = 'aeiouy'\n",
        "    word = word.lower()\n",
        "    if word[0] in vowels:\n",
        "        count +=1\n",
        "    for index in range(1,len(word)):\n",
        "        if word[index] in vowels and word[index-1] not in vowels:\n",
        "            count +=1\n",
        "    if word.endswith('e'):\n",
        "        count -= 1\n",
        "    if word.endswith('le'):\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "d = cmudict.dict()\n",
        "\n",
        "def nsyl(word):\n",
        "    try:\n",
        "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
        "    except KeyError:\n",
        "        #if word not found in cmudict\n",
        "        return syllables(word)\n",
        "\n",
        "#complex words\n",
        "def ComplexWords(web_words):\n",
        "    l=[]\n",
        "    s=set()\n",
        "    for i in range(len(web_words)):\n",
        "        l.append(nsyl(web_words[i]))\n",
        "        if l[i]>=2:\n",
        "            s.add(web_words[i])\n",
        "    return s\n",
        "\n",
        "def ComplexWordPercentage(web_words):\n",
        "    complex_words_percentage=len(ComplexWords(web_words))/len(web_words)*100\n",
        "    return round(complex_words_percentage,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KbmdnW-wna1"
      },
      "source": [
        ">Fog Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2Ubo0LVSwna1"
      },
      "outputs": [],
      "source": [
        "#Fog Index\n",
        "def FogIndex(Avg_sent_len,complex_words_percentage):\n",
        "    fog_index=0.4*(Avg_sent_len+complex_words_percentage)\n",
        "    return fog_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3r4hyGswna1"
      },
      "source": [
        "# 5. Word Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6QgOTRyewna2"
      },
      "outputs": [],
      "source": [
        "#word count using nltk library\n",
        "def nltk_word_count(web_article_cleaned):\n",
        "    nltk_stop_words=stopwords.words('english')\n",
        "    cleaned_words=[]\n",
        "    for i in web_article_cleaned:\n",
        "        if i not in nltk_stop_words:\n",
        "            cleaned_words.append(i)\n",
        "    return len(cleaned_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8V3yPj6wna2"
      },
      "source": [
        "# 6.Syllable Count Per Word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6bNCyxzCwna2"
      },
      "outputs": [],
      "source": [
        "def SyllableCount(words):\n",
        "    sc=0\n",
        "    for word in words:\n",
        "        if len(word)>2 and (word[-2:]=='es' or word[-2:]=='ed'):\n",
        "            continue\n",
        "        for char in word:\n",
        "            if char in ['a','e','i','o','u']:\n",
        "                sc+=1\n",
        "    return sc/len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFVYgDJjwna2"
      },
      "source": [
        "# 7.Personal Pronouns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bNKlW-cQwna2"
      },
      "outputs": [],
      "source": [
        "def personal_pronouns(web_article):\n",
        "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
        "    pronouns = pronounRegex.findall(web_article)\n",
        "    return len(pronouns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDKprAq3wna2"
      },
      "source": [
        "# 8.Average Word Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JngdrMJQwna2"
      },
      "outputs": [],
      "source": [
        "def Avg_word_len(web_article_cleaned):\n",
        "    sum_of_len=0\n",
        "    for word in web_article_cleaned:\n",
        "        sum_of_len+=len(word)\n",
        "    return sum_of_len/len(web_article_cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9YYLmPBSwna2"
      },
      "outputs": [],
      "source": [
        "if __name__=='__main__':\n",
        "    headers = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }\n",
        "    urls=pd.read_csv('Input.xlsx.csv')\n",
        "    n=len(urls)\n",
        "    # initializing all VARIABLES\n",
        "    P_Score=np.full(n,fill_value=None)\n",
        "    N_Score=np.full(n,fill_value=None)\n",
        "    Polarity_Score=np.full(n,fill_value=None)\n",
        "    Subj_Score=np.full(n,fill_value=None)\n",
        "    Avg_Sent_Len=np.full(n,fill_value=None)\n",
        "    P_complex=np.full(n,fill_value=None)\n",
        "    Fog_Index=np.full(n,fill_value=None)\n",
        "    Avg_Num_words_Per_Sent=np.full(n,fill_value=None)\n",
        "    Complex_Word_Count=np.full(n,fill_value=None)\n",
        "    Word_Count=np.full(n,fill_value=None)\n",
        "    Syllable_Count=np.full(n,fill_value=None)\n",
        "    Pronoun_Count=np.full(n,fill_value=None)\n",
        "    Avg_Word_Len=np.full(n,fill_value=None)\n",
        "\n",
        "    stop_words=stop_words_f()\n",
        "    pos_words,pos=Postive_words(stop_words)\n",
        "    neg_words,neg=Negative_words(stop_words)\n",
        "    for i in range(len(urls)):\n",
        "        id=urls['URL_ID'][i]\n",
        "        url=urls['URL'][i]\n",
        "        add=\"data\"+id+\".html\"\n",
        "        if requests.get(url, headers=headers).status_code!=404:\n",
        "            fectchSaveToFile(url,add)\n",
        "            web_article,og_web=WebArticle(add)\n",
        "            cleaned=Remove_stop_words(web_article,stop_words)\n",
        "            #Sentiment Analysis\n",
        "            p_score,n_score=positive_negative_score(cleaned,pos,neg)\n",
        "            P_Score[i]=p_score\n",
        "            N_Score[i]=n_score\n",
        "            polarity_score=Polarity_Scroe(p_score,n_score)\n",
        "            Polarity_Score[i]=polarity_score\n",
        "            subj_score=SubjectivityScore(p_score,n_score,cleaned)\n",
        "            Subj_Score[i]=subj_score\n",
        "            #Analysis Of Readability\n",
        "            avg_sent_len=AverageSentLength(web_article,og_web)\n",
        "            Avg_Sent_Len[i]=avg_sent_len\n",
        "            complex_words_percentage=ComplexWordPercentage(web_article)\n",
        "            P_complex[i]=complex_words_percentage\n",
        "            fog_index=FogIndex(avg_sent_len,complex_words_percentage)\n",
        "            Fog_Index[i]=fog_index\n",
        "            #Average Number of Words Per Sentence\n",
        "            Avg_Num_words_Per_Sent[i]=avg_sent_len\n",
        "            #Complex Word Count\n",
        "            complex_word_count=len(ComplexWords(web_article))\n",
        "            Complex_Word_Count[i]=complex_word_count\n",
        "            #Word Count\n",
        "            word_count=nltk_word_count(web_article)\n",
        "            Word_Count[i]=word_count\n",
        "            #Syllables Count PEr Word\n",
        "            syllablecount=SyllableCount(web_article)\n",
        "            Syllable_Count[i]=syllablecount\n",
        "            #Personal Pronouns\n",
        "            pronouns=personal_pronouns(og_web)\n",
        "            Pronoun_Count[i]=pronouns\n",
        "            #Average Word Length\n",
        "            avg_word_len=Avg_word_len(web_article)\n",
        "            Avg_Word_Len[i]=avg_word_len\n",
        "\n",
        "    urls['POSITIVE SCORE']=P_Score\n",
        "    urls['NEGATIVE SCORE']=N_Score\n",
        "    urls['POLARITY SCORE']=Polarity_Score\n",
        "    urls['SUBJECTIVITY SCORE']=Subj_Score\n",
        "    urls['AVG SENTENCE LENGTH']=Avg_Num_words_Per_Sent\n",
        "    urls['PERCENTAGE OF COMPLEX WORDS']=P_complex\n",
        "    urls['FOG INDEX']=Fog_Index\n",
        "    urls['AVG NUMBER OF WORDS PER SENTENCE']=Avg_Num_words_Per_Sent\n",
        "    urls['COMPLEX WORD COUNT']=Complex_Word_Count\n",
        "    urls['WORD COUNT']=Word_Count\n",
        "    urls['SYLLABLE PER WORD']=Syllable_Count\n",
        "    urls['PERSONAL PRONOUNS']=Pronoun_Count\n",
        "    urls['AVG WORD LENGTH']=Avg_Word_Len\n",
        "\n",
        "    submission=urls[['URL_ID','URL','POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE',\\\n",
        "                     'AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX','AVG NUMBER OF WORDS PER SENTENCE',\\\n",
        "                       'COMPLEX WORD COUNT', 'WORD COUNT','SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rHiu-WZd7_LC"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('Output Data Strucute.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdAmSkwIwna3"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
